[optimizer]
step_rate=18
learning_rate=0.0016024383719591365
gamma=0.6604875280627167
min_lr=0.0001
type=adam

[architecture]
d_model=92
nhead=17
dropout=0.7786303494059407
max_len=2993
num_decoder_layers=2
num_encoder_layers=4
dim_feedforward=523
activation=selu

[pipeline]
num_epochs=50

[loss]
p=2.0

[dataloader]
batch_size=1

