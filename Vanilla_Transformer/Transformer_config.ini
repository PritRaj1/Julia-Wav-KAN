[optimizer]
step_rate=26
learning_rate=0.001750014159
gamma=0.8323534063446
min_lr=0.0001
type=adam

[architecture]
d_model=128
nhead=16
dropout=0.1
max_len=5000
num_decoder_layers=8
num_encoder_layers=1
dim_feedforward=1024
activation=relu

[pipeline]
num_epochs=50

[loss]
p=2.0

[dataloader]
batch_size=5

